{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosko/miniconda3/envs/temp_lm_eval/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import qgrid\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, wasserstein_distance\n",
    "import json, bz2, pickle\n",
    "from pprint import pprint\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2471"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with bz2.open('./convai2_results.pickle.bz2') as fin:\n",
    "    convai2_data = pickle.load(fin)\n",
    "len(convai2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2759ebd849d741968146898feb1dd681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2471.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dialogue_scores = list()\n",
    "indices = list()\n",
    "dialogue_data = dict()\n",
    "\n",
    "for d in tqdm(convai2_data):\n",
    "    d_item = dict()\n",
    "    dialogue_data[str(d['dialog_id'])] = d\n",
    "    indices.append(str(d['dialog_id']))\n",
    "    d_item['quality'] = d['quality']\n",
    "    \n",
    "    pred_keys = list(d['predictions'].keys())\n",
    "    bert_keys = list(filter(lambda x: 'bert' in x, pred_keys))\n",
    "    \n",
    "    for pred_key in bert_keys:\n",
    "        pred_sum = sum([np.log(x) for x in d['predictions'][pred_key] if x != 0])\n",
    "        pred_avg = pred_sum / len(d['predictions'][pred_key])\n",
    "        d_item['{}_log_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_log_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        pred_sum = sum(d['predictions'][pred_key])\n",
    "        pred_avg = pred_sum / len(d['predictions'][pred_key])\n",
    "        d_item['{}_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        d_item['{}_prd'.format(pred_key)] = np.prod(d['predictions'][pred_key])        \n",
    "        d_item['{}_prd_avg'.format(pred_key)] = d_item['{}_prd'.format(pred_key)] / len(d['predictions'][pred_key])\n",
    "        \n",
    "    prob_keys = list(filter(lambda x: 'prob' in x and x not in bert_keys, pred_keys))\n",
    "    \n",
    "    for pred_key in prob_keys: \n",
    "        s_sums = [sum(x) for x in d['predictions'][pred_key]]\n",
    "        s_sums_d_sum = sum([x for x in s_sums if x != 0])\n",
    "        s_sums_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_sums_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_sums_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_sums = [sum([np.log(x_1) for x_1 in x if x_1 != 0]) for x in d['predictions'][pred_key]]\n",
    "        s_sums_d_sum = sum([x for x in s_sums if x != 0])\n",
    "        s_sums_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_log_sums_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_log_sums_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_prd = [np.prod(x) for x in d['predictions'][pred_key]]\n",
    "        s_prd_d_sum = sum(s_sums)\n",
    "        s_prd_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_prod_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_prod_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_avg = [float(sum([np.log(x_1) for x_1 in x if x_1 != 0]) / len(x)) for x in d['predictions'][pred_key] if len(x) > 0]        \n",
    "        s_avg_d_sum = sum(s_avg)\n",
    "        s_avg_d_avg = s_avg_d_sum / len(s_avg)        \n",
    "        \n",
    "        d_item['{}_s_log_avg_d_sum'.format(pred_key)] = s_avg_d_sum\n",
    "        d_item['{}_s_log_avg_d_avg'.format(pred_key)] = s_avg_d_avg\n",
    "        \n",
    "        s_avg = [float(sum(x) / len(x)) for x in d['predictions'][pred_key] if len(x) > 0]        \n",
    "        s_avg_d_sum = sum(s_avg)\n",
    "        s_avg_d_avg = s_avg_d_sum / len(s_avg)        \n",
    "        \n",
    "        d_item['{}_s_avg_d_sum'.format(pred_key)] = s_avg_d_sum\n",
    "        d_item['{}_s_avg_d_avg'.format(pred_key)] = s_avg_d_avg\n",
    "        \n",
    "    dialogue_scores.append(d_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0xab38710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.881846</td>\n",
       "      <td>-0.388185</td>\n",
       "      <td>9.020201</td>\n",
       "      <td>0.902020</td>\n",
       "      <td>0.020613</td>\n",
       "      <td>2.061273e-03</td>\n",
       "      <td>-98.081533</td>\n",
       "      <td>-9.808153</td>\n",
       "      <td>0.979799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-16.465518</td>\n",
       "      <td>-1.646552</td>\n",
       "      <td>2.374640</td>\n",
       "      <td>0.237464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7fcf7907</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.026552</td>\n",
       "      <td>-0.003319</td>\n",
       "      <td>7.973796</td>\n",
       "      <td>0.996725</td>\n",
       "      <td>0.973798</td>\n",
       "      <td>1.217247e-01</td>\n",
       "      <td>-86.976495</td>\n",
       "      <td>-10.872062</td>\n",
       "      <td>0.026204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>-11.353486</td>\n",
       "      <td>-1.419186</td>\n",
       "      <td>2.444737</td>\n",
       "      <td>0.305592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7ebe8afe</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.814247</td>\n",
       "      <td>-0.488390</td>\n",
       "      <td>14.129607</td>\n",
       "      <td>0.883100</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>2.524617e-05</td>\n",
       "      <td>-132.596779</td>\n",
       "      <td>-8.287299</td>\n",
       "      <td>1.870392</td>\n",
       "      <td>...</td>\n",
       "      <td>1.932181</td>\n",
       "      <td>0.120761</td>\n",
       "      <td>1.932181</td>\n",
       "      <td>0.120761</td>\n",
       "      <td>1.932181</td>\n",
       "      <td>0.120761</td>\n",
       "      <td>-23.252595</td>\n",
       "      <td>-1.453287</td>\n",
       "      <td>4.700146</td>\n",
       "      <td>0.293759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7d519415</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>13.995970</td>\n",
       "      <td>0.999712</td>\n",
       "      <td>0.995975</td>\n",
       "      <td>7.114108e-02</td>\n",
       "      <td>-146.911929</td>\n",
       "      <td>-10.493709</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>-18.444747</td>\n",
       "      <td>-1.317482</td>\n",
       "      <td>4.799712</td>\n",
       "      <td>0.342837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d81519</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.826997</td>\n",
       "      <td>-1.425222</td>\n",
       "      <td>6.971820</td>\n",
       "      <td>0.774647</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.985819e-07</td>\n",
       "      <td>-56.654462</td>\n",
       "      <td>-6.294940</td>\n",
       "      <td>2.028179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992469</td>\n",
       "      <td>0.110274</td>\n",
       "      <td>0.992469</td>\n",
       "      <td>0.110274</td>\n",
       "      <td>0.992469</td>\n",
       "      <td>0.110274</td>\n",
       "      <td>-15.517803</td>\n",
       "      <td>-1.724200</td>\n",
       "      <td>2.266936</td>\n",
       "      <td>0.251882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            quality  bert-base-uncased_nsp_0_log_sum  \\\n",
       "0xab38710       1.0                        -3.881846   \n",
       "0x7fcf7907      1.0                        -0.026552   \n",
       "0x7ebe8afe      2.0                        -7.814247   \n",
       "0x7d519415      4.0                        -0.004033   \n",
       "0x1d81519       1.0                       -12.826997   \n",
       "\n",
       "            bert-base-uncased_nsp_0_log_avg  bert-base-uncased_nsp_0_sum  \\\n",
       "0xab38710                         -0.388185                     9.020201   \n",
       "0x7fcf7907                        -0.003319                     7.973796   \n",
       "0x7ebe8afe                        -0.488390                    14.129607   \n",
       "0x7d519415                        -0.000288                    13.995970   \n",
       "0x1d81519                         -1.425222                     6.971820   \n",
       "\n",
       "            bert-base-uncased_nsp_0_avg  bert-base-uncased_nsp_0_prd  \\\n",
       "0xab38710                      0.902020                     0.020613   \n",
       "0x7fcf7907                     0.996725                     0.973798   \n",
       "0x7ebe8afe                     0.883100                     0.000404   \n",
       "0x7d519415                     0.999712                     0.995975   \n",
       "0x1d81519                      0.774647                     0.000003   \n",
       "\n",
       "            bert-base-uncased_nsp_0_prd_avg  bert-base-uncased_nsp_1_log_sum  \\\n",
       "0xab38710                      2.061273e-03                       -98.081533   \n",
       "0x7fcf7907                     1.217247e-01                       -86.976495   \n",
       "0x7ebe8afe                     2.524617e-05                      -132.596779   \n",
       "0x7d519415                     7.114108e-02                      -146.911929   \n",
       "0x1d81519                      2.985819e-07                       -56.654462   \n",
       "\n",
       "            bert-base-uncased_nsp_1_log_avg  bert-base-uncased_nsp_1_sum  ...  \\\n",
       "0xab38710                         -9.808153                     0.979799  ...   \n",
       "0x7fcf7907                       -10.872062                     0.026204  ...   \n",
       "0x7ebe8afe                        -8.287299                     1.870392  ...   \n",
       "0x7d519415                       -10.493709                     0.004030  ...   \n",
       "0x1d81519                         -6.294940                     2.028179  ...   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_sums_d_sum  \\\n",
       "0xab38710                                            0.002793   \n",
       "0x7fcf7907                                           0.001093   \n",
       "0x7ebe8afe                                           1.932181   \n",
       "0x7d519415                                           0.001678   \n",
       "0x1d81519                                            0.992469   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_sums_d_avg  \\\n",
       "0xab38710                                            0.000279   \n",
       "0x7fcf7907                                           0.000137   \n",
       "0x7ebe8afe                                           0.120761   \n",
       "0x7d519415                                           0.000120   \n",
       "0x1d81519                                            0.110274   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_sums_d_sum  \\\n",
       "0xab38710                                            0.002793       \n",
       "0x7fcf7907                                           0.001093       \n",
       "0x7ebe8afe                                           1.932181       \n",
       "0x7d519415                                           0.001678       \n",
       "0x1d81519                                            0.992469       \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_sums_d_avg  \\\n",
       "0xab38710                                            0.000279       \n",
       "0x7fcf7907                                           0.000137       \n",
       "0x7ebe8afe                                           0.120761       \n",
       "0x7d519415                                           0.000120       \n",
       "0x1d81519                                            0.110274       \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_prod_d_sum  \\\n",
       "0xab38710                                            0.002793   \n",
       "0x7fcf7907                                           0.001093   \n",
       "0x7ebe8afe                                           1.932181   \n",
       "0x7d519415                                           0.001678   \n",
       "0x1d81519                                            0.992469   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_prod_d_avg  \\\n",
       "0xab38710                                            0.000279   \n",
       "0x7fcf7907                                           0.000137   \n",
       "0x7ebe8afe                                           0.120761   \n",
       "0x7d519415                                           0.000120   \n",
       "0x1d81519                                            0.110274   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_avg_d_sum  \\\n",
       "0xab38710                                          -16.465518      \n",
       "0x7fcf7907                                         -11.353486      \n",
       "0x7ebe8afe                                         -23.252595      \n",
       "0x7d519415                                         -18.444747      \n",
       "0x1d81519                                          -15.517803      \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_avg_d_avg  \\\n",
       "0xab38710                                           -1.646552      \n",
       "0x7fcf7907                                          -1.419186      \n",
       "0x7ebe8afe                                          -1.453287      \n",
       "0x7d519415                                          -1.317482      \n",
       "0x1d81519                                           -1.724200      \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_avg_d_sum  \\\n",
       "0xab38710                                           2.374640   \n",
       "0x7fcf7907                                          2.444737   \n",
       "0x7ebe8afe                                          4.700146   \n",
       "0x7d519415                                          4.799712   \n",
       "0x1d81519                                           2.266936   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_avg_d_avg  \n",
       "0xab38710                                           0.237464  \n",
       "0x7fcf7907                                          0.305592  \n",
       "0x7ebe8afe                                          0.293759  \n",
       "0x7d519415                                          0.342837  \n",
       "0x1d81519                                           0.251882  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_scores = pd.DataFrame(dialogue_scores)\n",
    "dialogue_scores.index = indices\n",
    "dialogue_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2471, 125)\n",
      "(2279, 125)\n"
     ]
    }
   ],
   "source": [
    "print(dialogue_scores.shape)\n",
    "dialogue_scores = dialogue_scores.dropna()\n",
    "print(dialogue_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0xab38710</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.990521</td>\n",
       "      <td>0.943470</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>0.901918</td>\n",
       "      <td>0.020613</td>\n",
       "      <td>4.122574e-03</td>\n",
       "      <td>0.988179</td>\n",
       "      <td>0.222702</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.989993</td>\n",
       "      <td>0.582809</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>0.353607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7fcf7907</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.008846</td>\n",
       "      <td>0.996724</td>\n",
       "      <td>0.973804</td>\n",
       "      <td>2.434511e-01</td>\n",
       "      <td>0.989517</td>\n",
       "      <td>0.138378</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.993470</td>\n",
       "      <td>0.706141</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.503101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7ebe8afe</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.980918</td>\n",
       "      <td>0.928877</td>\n",
       "      <td>0.015676</td>\n",
       "      <td>0.882978</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>5.049267e-05</td>\n",
       "      <td>0.984019</td>\n",
       "      <td>0.343243</td>\n",
       "      <td>0.019124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.142464</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.142464</td>\n",
       "      <td>0.019770</td>\n",
       "      <td>0.142464</td>\n",
       "      <td>0.985376</td>\n",
       "      <td>0.687643</td>\n",
       "      <td>0.015609</td>\n",
       "      <td>0.477136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x7d519415</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.995982</td>\n",
       "      <td>1.422831e-01</td>\n",
       "      <td>0.982294</td>\n",
       "      <td>0.168366</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.988647</td>\n",
       "      <td>0.761309</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.584827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1d81519</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.968677</td>\n",
       "      <td>0.792449</td>\n",
       "      <td>0.007734</td>\n",
       "      <td>0.774407</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>5.971678e-07</td>\n",
       "      <td>0.993172</td>\n",
       "      <td>0.501155</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.010155</td>\n",
       "      <td>0.130092</td>\n",
       "      <td>0.990638</td>\n",
       "      <td>0.540690</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.385244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            quality  bert-base-uncased_nsp_0_log_sum  \\\n",
       "0xab38710      0.00                         0.990521   \n",
       "0x7fcf7907     0.00                         0.999935   \n",
       "0x7ebe8afe     0.25                         0.980918   \n",
       "0x7d519415     0.75                         0.999990   \n",
       "0x1d81519      0.00                         0.968677   \n",
       "\n",
       "            bert-base-uncased_nsp_0_log_avg  bert-base-uncased_nsp_0_sum  \\\n",
       "0xab38710                          0.943470                     0.010007   \n",
       "0x7fcf7907                         0.999517                     0.008846   \n",
       "0x7ebe8afe                         0.928877                     0.015676   \n",
       "0x7d519415                         0.999959                     0.015528   \n",
       "0x1d81519                          0.792449                     0.007734   \n",
       "\n",
       "            bert-base-uncased_nsp_0_avg  bert-base-uncased_nsp_0_prd  \\\n",
       "0xab38710                      0.901918                     0.020613   \n",
       "0x7fcf7907                     0.996724                     0.973804   \n",
       "0x7ebe8afe                     0.882978                     0.000404   \n",
       "0x7d519415                     0.999715                     0.995982   \n",
       "0x1d81519                      0.774407                     0.000003   \n",
       "\n",
       "            bert-base-uncased_nsp_0_prd_avg  bert-base-uncased_nsp_1_log_sum  \\\n",
       "0xab38710                      4.122574e-03                         0.988179   \n",
       "0x7fcf7907                     2.434511e-01                         0.989517   \n",
       "0x7ebe8afe                     5.049267e-05                         0.984019   \n",
       "0x7d519415                     1.422831e-01                         0.982294   \n",
       "0x1d81519                      5.971678e-07                         0.993172   \n",
       "\n",
       "            bert-base-uncased_nsp_1_log_avg  bert-base-uncased_nsp_1_sum  ...  \\\n",
       "0xab38710                          0.222702                     0.010018  ...   \n",
       "0x7fcf7907                         0.138378                     0.000268  ...   \n",
       "0x7ebe8afe                         0.343243                     0.019124  ...   \n",
       "0x7d519415                         0.168366                     0.000041  ...   \n",
       "0x1d81519                          0.501155                     0.020738  ...   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_sums_d_sum  \\\n",
       "0xab38710                                            0.000029   \n",
       "0x7fcf7907                                           0.000011   \n",
       "0x7ebe8afe                                           0.019770   \n",
       "0x7d519415                                           0.000017   \n",
       "0x1d81519                                            0.010155   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_sums_d_avg  \\\n",
       "0xab38710                                            0.000327   \n",
       "0x7fcf7907                                           0.000159   \n",
       "0x7ebe8afe                                           0.142464   \n",
       "0x7d519415                                           0.000139   \n",
       "0x1d81519                                            0.130092   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_sums_d_sum  \\\n",
       "0xab38710                                            0.000029       \n",
       "0x7fcf7907                                           0.000011       \n",
       "0x7ebe8afe                                           0.019770       \n",
       "0x7d519415                                           0.000017       \n",
       "0x1d81519                                            0.010155       \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_sums_d_avg  \\\n",
       "0xab38710                                            0.000327       \n",
       "0x7fcf7907                                           0.000159       \n",
       "0x7ebe8afe                                           0.142464       \n",
       "0x7d519415                                           0.000139       \n",
       "0x1d81519                                            0.130092       \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_prod_d_sum  \\\n",
       "0xab38710                                            0.000029   \n",
       "0x7fcf7907                                           0.000011   \n",
       "0x7ebe8afe                                           0.019770   \n",
       "0x7d519415                                           0.000017   \n",
       "0x1d81519                                            0.010155   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_prod_d_avg  \\\n",
       "0xab38710                                            0.000327   \n",
       "0x7fcf7907                                           0.000159   \n",
       "0x7ebe8afe                                           0.142464   \n",
       "0x7d519415                                           0.000139   \n",
       "0x1d81519                                            0.130092   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_avg_d_sum  \\\n",
       "0xab38710                                            0.989993      \n",
       "0x7fcf7907                                           0.993470      \n",
       "0x7ebe8afe                                           0.985376      \n",
       "0x7d519415                                           0.988647      \n",
       "0x1d81519                                            0.990638      \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_log_avg_d_avg  \\\n",
       "0xab38710                                            0.582809      \n",
       "0x7fcf7907                                           0.706141      \n",
       "0x7ebe8afe                                           0.687643      \n",
       "0x7d519415                                           0.761309      \n",
       "0x1d81519                                            0.540690      \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_avg_d_sum  \\\n",
       "0xab38710                                           0.007627   \n",
       "0x7fcf7907                                          0.007867   \n",
       "0x7ebe8afe                                          0.015609   \n",
       "0x7d519415                                          0.015951   \n",
       "0x1d81519                                           0.007257   \n",
       "\n",
       "            gpt2-large_sentences_best_word_probs_s_avg_d_avg  \n",
       "0xab38710                                           0.353607  \n",
       "0x7fcf7907                                          0.503101  \n",
       "0x7ebe8afe                                          0.477136  \n",
       "0x7d519415                                          0.584827  \n",
       "0x1d81519                                           0.385244  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in dialogue_scores.columns:\n",
    "    dialogue_scores[col] = minmax_scale(dialogue_scores[col])\n",
    "\n",
    "dialogue_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c786ea1c28ee47a09cebbbc7f2806ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': False, 'defa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((predictions-targets)**2))\n",
    "\n",
    "all_scores = {col:dict() for col in dialogue_scores.columns[1:]}\n",
    "\n",
    "for col in dialogue_scores.columns[1:]:\n",
    "    for f in (pearsonr, spearmanr, wasserstein_distance, rmse):\n",
    "        scores = f(dialogue_scores.quality, dialogue_scores[col])\n",
    "        if np.isscalar(scores):\n",
    "            scores = [scores]\n",
    "        \n",
    "        for score, name in zip(scores, [f.__name__, f.__name__+'_p']):\n",
    "            all_scores[col][name] = round(score, 3)\n",
    "\n",
    "all_scores = pd.DataFrame.from_dict(all_scores, orient='index')\n",
    "qgrid.show_grid(all_scores,\n",
    "               grid_options={\n",
    "    # SlickGrid options\n",
    "    'fullWidthRows': True,\n",
    "    'syncColumnCellResize': True,\n",
    "    'forceFitColumns': False,\n",
    "    'defaultColumnWidth': 80,\n",
    "    'rowHeight': 28,\n",
    "    'enableColumnReorder': False,\n",
    "    'enableTextSelectionOnCells': True,\n",
    "    'editable': True,\n",
    "    'autoEdit': False,\n",
    "    'explicitInitialization': True,\n",
    "\n",
    "    # Qgrid options\n",
    "    'maxVisibleRows': 15,\n",
    "    'minVisibleRows': 8,\n",
    "    'sortable': True,\n",
    "    'filterable': True,\n",
    "    'highlightSelectedCell': False,\n",
    "    'highlightSelectedRow': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[(0, 'Yo!'),\n",
      " (1, 'Hi!'),\n",
      " (2, 'Hi! i am in alaska.'),\n",
      " (3, 'Cool! What are you doing there?'),\n",
      " (4, 'I am a bit of a yoga instructor.'),\n",
      " (5, 'Wow! Are you man or woman?'),\n",
      " (6, 'I am! i am a woman.'),\n",
      " (7, 'How nice!  Do you have a boyfriend?'),\n",
      " (8, 'I do not. i am a single mom.'),\n",
      " (9, 'Sad. How old are you?'),\n",
      " (10, 'I am 25. i am a bit older than you.'),\n",
      " (11, \"No, I'am older! Do you have any hobbies?\"),\n",
      " (12, 'I like to read and write. i love to read.'),\n",
      " (13, 'What books do you like to read?'),\n",
      " (14, 'I like to read a lot'),\n",
      " (15, 'What books?'),\n",
      " (16, \"I don't know.\"),\n",
      " (17, \"I don't believe you.\"),\n",
      " (18, 'I am sorry to hear that. i am a bit of a loner.')]\n",
      "{'gpt2-large_sentences_best_words': [(0, '!__ĠI'),\n",
      "                                     (1, \"!__Ċ__'m__Ġa__Ġthe__aska__Ġand__Ġi\"),\n",
      "                                     (2, '!__ĠI__Ġis__Ġyou__Ġdoing__Ġin__?__Ċ'),\n",
      "                                     (3,\n",
      "                                      \"'m__Ġa__Ġstudent__Ġof__Ġa__Ġfan__Ġteacher__,__ĠI\"),\n",
      "                                     (4,\n",
      "                                      ',__ĠI__Ġyou__Ġkidding__Ġenough__Ġwoman__?__ĠI'),\n",
      "                                     (5, \"'m__Ġa__ĠI__Ġam__Ġa__Ġman__!__ĠI\"),\n",
      "                                     (6,\n",
      "                                      'Ġcan__Ġof__ĠI__Âł__Ġyou__Ġknow__Ġa__Ġpicture__?__ĠDo'),\n",
      "                                     (7,\n",
      "                                      \"'m__.__.__ĠI__Ġam__Ġa__Ġvirgin__Ġwoman__.__Ġi\"),\n",
      "                                     (8, 'Ġto__Ċ__Ġcan__Ġare__Ġyou__?__Ċ'),\n",
      "                                     (9,\n",
      "                                      \"'m__Ġa__.__ĠI__Ġhave__Ġa__Ġstudent__Ġolder__Ġthan__Ġyou__.__ĠI\"),\n",
      "                                     (10,\n",
      "                                      ',__Ġi__Ġam__M__Ġnot__Ġthan__ĠI__Ġyou__Ġknow__Ġa__Ġidea__?__ĠI'),\n",
      "                                     (11,\n",
      "                                      \"'m__Ġto__Ġread__,__Ġwrite__.__ĠI__Ġlike__Ġto__Ġplay__Ġand__Ġi\"),\n",
      "                                     (12,\n",
      "                                      'Ġis__Ġdo__Ġyou__Ġlike__Ġto__Ġread__?__I'),\n",
      "                                     (13, 'Ġlike__Ġto__Ġread__Ġa__Ġlot__Ġof'),\n",
      "                                     (14, 'ĠI__Ġdo__I'),\n",
      "                                     (15, \"'m__'t__Ġknow__.__ĠI\"),\n",
      "                                     (16, \"Ġdon__'t__Ġknow__Ġin__.__I\"),\n",
      "                                     (17,\n",
      "                                      'Ġdon__Ġnot__Ġto__Ġsay__Ġthat__.__ĠI__Ġam__Ġnot__Ġvery__Ġconfused__Ġa__Ġskept__oner__.__Ġi')],\n",
      " 'gpt2-medium_sentences_best_words': [(0, '!__ĠI'),\n",
      "                                      (1,\n",
      "                                       \"!__Ċ__'m__Ġa__Ġthe__abama__Ġand__ĠI\"),\n",
      "                                      (2, '!__ĠI__Ġis__Ġyou__Ġdoing__?__?__ĠI'),\n",
      "                                      (3,\n",
      "                                       \"'m__Ġnot__Ġfan__Ġconfused__Ġa__Ġnerd__Ġfanatic__.__ĠI\"),\n",
      "                                      (4,\n",
      "                                       ',__ĠI__Ġyou__Ġkidding__Ġor__Ġwoman__?__ĠI'),\n",
      "                                      (5, \"'m__Ġa__ĠI__Ġam__Ġa__Ġman__.__ĠI\"),\n",
      "                                      (6,\n",
      "                                       'Ġcan__!__ĠI__Âł__Ġyou__Ġknow__Ġany__Ġboyfriend__?__Ġ'),\n",
      "                                      (7,\n",
      "                                       \"'m__.__.__Ġ__Ġam__Ġnot__Ġsingle__Ġwoman__.__ĠI\"),\n",
      "                                      (8, 'Ġto__ĠI__Ġcan__Ġare__Ġyou__?__ĠI'),\n",
      "                                      (9,\n",
      "                                       \"'m__Ġ17__.__ĠI__Ġam__Ġa__Ġstudent__Ġolder__Ġthan__Ġyou__.__ĠI\"),\n",
      "                                      (10,\n",
      "                                       ',__ĠI__Ġam__M__Ġnot__Ġthan__ĠI__Ġyou__Ġknow__Ġany__Ġidea__?__I'),\n",
      "                                      (11,\n",
      "                                       \"'m__Ġto__Ġread__,__Ġwrite__.__ĠI__'m__Ġto__Ġplay__Ġand__Ġi\"),\n",
      "                                      (12,\n",
      "                                       'Ġi__Ġdo__Ġyou__Ġlike__Ġto__Ġread__?__Ċ'),\n",
      "                                      (13,\n",
      "                                       'Ġlike__Ġto__Ġread__Ġbooks__Ġlot__Ġof'),\n",
      "                                      (14, \"'s__Ġdo__ĠI\"),\n",
      "                                      (15, \"'m__'t__Ġknow__.__ĠI\"),\n",
      "                                      (16, \"'m__'t__Ġknow__Ġin__.__I\"),\n",
      "                                      (17,\n",
      "                                       'Ġdon__Ġnot__.__Ġhear__Ġthat__.__ĠI__Ġam__Ġnot__Ġvery__Ġconfused__Ġa__Ġfan__oner__,__Ġi')],\n",
      " 'gpt2_sentences_best_words': [(0, '!__ĠI'),\n",
      "                               (1, \"!__Ċ__'m__Ġa__Ġthe__abama__Ġand__Ġi\"),\n",
      "                               (2, '!__Ġi__Ġis__Ġyou__Ġdoing__Ġhere__?__I'),\n",
      "                               (3,\n",
      "                                \"'m__Ġa__Ġstudent__Ġof__Ġa__Ġfan__Ġteacher__Ġand__ĠI\"),\n",
      "                               (4, ',__ĠI__Ġyou__Ġa__?__Ġwoman__?__ĠI'),\n",
      "                               (5, \"'m__Ġa__ĠI__Ġam__Ġa__Ġman__!__Ġi\"),\n",
      "                               (6,\n",
      "                                'Ġcan__Ġof__ĠI__Âł__Ġyou__Ġthink__Ġany__Ġboyfriend__?__Ġ'),\n",
      "                               (7,\n",
      "                                \"'m__.__.__ĠI__Ġam__Ġnot__Ġvery__Ġman__.__Ġi\"),\n",
      "                               (8, 'Ġto__Ċ__Ġcan__Ġare__Ġyou__?__Ċ'),\n",
      "                               (9,\n",
      "                                \"'m__Ġa__Ġyears__ĠI__Ġam__Ġa__Ġstudent__Ġolder__Ġthan__Ġyou__.__Ġi\"),\n",
      "                               (10,\n",
      "                                ',__Ġi__Ġam__M__Ġnot__Ġthan__ĠI__Ġyou__Ġknow__Ġany__Ġidea__?__ĠI'),\n",
      "                               (11,\n",
      "                                \"'m__Ġto__Ġplay__,__Ġwrite__.__ĠI__'m__Ġto__Ġread__Ġand__ĠI\"),\n",
      "                               (12, 'ĠI__Ġdo__Ġyou__Ġread__Ġto__Ġread__?__Ċ'),\n",
      "                               (13, \"'ve__Ġto__Ġread__Ġbooks__Ġlot__Ġof\"),\n",
      "                               (14, 'ĠI__Ġare__Ċ'),\n",
      "                               (15, \"'ve__'t__Ġknow__.__ĠI\"),\n",
      "                               (16, \"'m__'t__Ġknow__Ġin__.__I\"),\n",
      "                               (17,\n",
      "                                'Ġdon__Ġnot__.__Ġhear__Ġthat__.__I__Ġam__Ġsorry__Ġlittle__Ġof__Ġa__Ġfan__oner__.__Ġi')],\n",
      " 'xlnet-base-cased_sentences_best_words': [(0, '<eop>__,'),\n",
      "                                           (1,\n",
      "                                            '.__▁Hi__▁Hi__!__▁__▁a__▁the__s__.'),\n",
      "                                           (2,\n",
      "                                            '<eod>__▁Cool__!__▁is__▁the__▁in__?__?'),\n",
      "                                           (3,\n",
      "                                            '<eop>__▁am__▁a__▁__▁a__▁a__▁bit__▁teacher__.'),\n",
      "                                           (4,\n",
      "                                            '<eop>__:__▁__!__▁you__▁a__▁__▁woman__▁or'),\n",
      "                                           (5,\n",
      "                                            '<eop>__s__▁a__▁__!__▁am__!__▁man__.'),\n",
      "                                           (6,\n",
      "                                            '<eop>__▁I__▁and__▁__▁Do__▁have__▁any__▁__?'),\n",
      "                                           (7,\n",
      "                                            '<eop>__▁am__▁not__▁have__▁Do__.__▁__▁a__▁__▁single__.'),\n",
      "                                           (8,\n",
      "                                            '<eop>__▁Sad__.__▁I__▁how__▁you__?'),\n",
      "                                           (9,\n",
      "                                            '▁__▁am__▁a__.__.__.__▁am__▁25__▁__▁shy__▁than__▁most__.'),\n",
      "                                           (10,\n",
      "                                            '▁I__▁No__▁No__▁am__m__▁a__▁than__▁No__▁you__▁think__▁a__▁of__?'),\n",
      "                                           (11,\n",
      "                                            '▁More__,__▁to__▁be__▁and__▁write__.__▁I__\"__▁like__▁to__▁play__▁and'),\n",
      "                                           (12,\n",
      "                                            '<eop>__▁What__▁are__▁do__▁think__▁to__▁read__▁and'),\n",
      "                                           (13,\n",
      "                                            '<eop>__▁do__▁to__▁read__▁the__▁lot'),\n",
      "                                           (14, '▁Maybe__▁What__▁are'),\n",
      "                                           (15, \"▁All__▁I__.__'__▁want__,\"),\n",
      "                                           (16, \"▁Art__▁I__'__t__▁know__.__.\"),\n",
      "                                           (17,\n",
      "                                            '▁__▁don__▁__,__▁to__▁that__▁I__▁I__i__▁don__▁sorry__▁__▁confused__▁a__▁__r__.')],\n",
      " 'xlnet-large-cased_sentences_best_words': [(0, '▁X__,'),\n",
      "                                            (1,\n",
      "                                             ',__,__▁__,__▁__▁the__▁the__d__!'),\n",
      "                                            (2,\n",
      "                                             '<eod>__▁Cool__▁Cool__!__▁you__▁doing__?__?'),\n",
      "                                            (3,\n",
      "                                             '▁a__▁I__▁of__▁car__▁on__▁a__▁million__▁and__,'),\n",
      "                                            (4,\n",
      "                                             '<eod>__\"__Wow__!__▁you__▁a__?__▁woman__?'),\n",
      "                                            (5,\n",
      "                                             '▁__▁am__▁man__▁__Wow__▁am__!__▁man__!'),\n",
      "                                            (6,\n",
      "                                             '<eod>__▁How__!__!__▁you__▁think__▁a__▁__?'),\n",
      "                                            (7,\n",
      "                                             '<eod>__▁I__▁not__▁have__.__.__▁do__▁a__▁__▁person__.'),\n",
      "                                            (8,\n",
      "                                             '<eod>__▁Sad__.__▁sad__▁are__▁you__?'),\n",
      "                                            (9,\n",
      "                                             '<eod>__▁think__▁the__,__.__.__▁am__▁a__▁__▁overweight__▁than__▁you__.'),\n",
      "                                            (10,\n",
      "                                             '▁__▁No__▁no__▁am__m__▁not__▁than__▁I__▁you__▁think__▁a__▁other__?'),\n",
      "                                            (11,\n",
      "                                             \"▁I__'__,__▁play__.__▁write__.__▁I__.__▁like__▁to__▁play__▁and\"),\n",
      "                                            (12,\n",
      "                                             '▁__▁I__▁are__▁you__▁like__▁to__▁read__▁and'),\n",
      "                                            (13, '▁of__,__,__▁you__▁to__▁few'),\n",
      "                                            (14, '▁or__▁I__▁I'),\n",
      "                                            (15, '▁x__,__▁of__▁not__▁pass__,'),\n",
      "                                            (16,\n",
      "                                             \"<eod>__▁I__'__t__▁know__▁in__.\"),\n",
      "                                            (17,\n",
      "                                             '<eod>__▁don__▁__▁to__▁to__▁hear__▁you__▁I__(__▁don__▁sorry__▁good__▁of__▁a__▁__r__▁and')]}\n",
      "{'bert-base-uncased_nsp_0_avg': [0.9997615691400308],\n",
      " 'bert-base-uncased_nsp_0_log_avg': [0.9999652617628297],\n",
      " 'bert-base-uncased_nsp_0_log_sum': [0.9999893845414174],\n",
      " 'bert-base-uncased_nsp_0_prd': [0.9956622782591821],\n",
      " 'bert-base-uncased_nsp_0_prd_avg': [0.110629142028798],\n",
      " 'bert-base-uncased_nsp_0_sum': [0.019966227136669887],\n",
      " 'bert-base-uncased_nsp_1_avg': [0.00023846692389901936],\n",
      " 'bert-base-uncased_nsp_1_log_avg': [0.1604640239403179],\n",
      " 'bert-base-uncased_nsp_1_log_sum': [0.9770182217454919],\n",
      " 'bert-base-uncased_nsp_1_prd': [1.5456237973743042e-83],\n",
      " 'bert-base-uncased_nsp_1_prd_avg': [1.717359774860338e-84],\n",
      " 'bert-base-uncased_nsp_1_sum': [4.4383648588487834e-05],\n",
      " 'bert-large-uncased_nsp_0_avg': [0.9373050699778162],\n",
      " 'bert-large-uncased_nsp_0_log_avg': [0.958372181114147],\n",
      " 'bert-large-uncased_nsp_0_log_sum': [0.9936952597470672],\n",
      " 'bert-large-uncased_nsp_0_prd': [0.045638086467186756],\n",
      " 'bert-large-uncased_nsp_0_prd_avg': [0.005070898496354084],\n",
      " 'bert-large-uncased_nsp_0_sum': [0.018578669352691625],\n",
      " 'bert-large-uncased_nsp_1_avg': [0.06269499191254252],\n",
      " 'bert-large-uncased_nsp_1_log_avg': [0.19852951344750813],\n",
      " 'bert-large-uncased_nsp_1_log_sum': [0.9781560499538936],\n",
      " 'bert-large-uncased_nsp_1_prd': [4.0551052827527126e-83],\n",
      " 'bert-large-uncased_nsp_1_prd_avg': [4.505672536391903e-84],\n",
      " 'bert-large-uncased_nsp_1_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_avg': [0.5558536112741211],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_sum': [0.019841791576138926],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_avg': [0.718660546441552],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_sum': [0.9840988608839046],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_avg': [0.0007145003461683575],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_sum': [0.004646222171143127],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_avg': [0.2855964557881232],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_sum': [0.9823395189139328],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_avg': [0.49309329936496815],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_sum': [0.01964359563091236],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_avg': [0.6775218759446114],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_sum': [0.9843255615745738],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_avg': [0.0007505299450180051],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_sum': [0.005212266456313433],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_avg': [0.28234971748513427],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_sum': [0.9821928344551898],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_avg': [0.5350304416823397],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_sum': [0.020446262232585714],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_avg': [0.7406336357258763],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_sum': [0.9847434560667256],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_avg': [0.001068645185935378],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_sum': [0.006565139679430149],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_avg': [0.2601429140199645],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_sum': [0.9825978557048153],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'quality': [1.0],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_avg': [0.5497711469868787],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_sum': [0.02006449501120293],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.7012840152070012],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9850389908404694],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_avg': [0.4912686107061585],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_sum': [0.0340831005129762],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_avg': [0.8124930238182636],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_sum': [0.9862168878938592],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_avg': [0.6320427459931954],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_sum': [0.02091956086945759],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.7315805752339269],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9852375874075905],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_avg': [0.4904689081796337],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_sum': [0.03745866931402425],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_avg': [0.8063181539372717],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_sum': [0.9859609658553825],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_sum': [0.009788047013711648],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_avg': [0.06269499191254252],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_sum': [0.009788047013711648]}\n"
     ]
    }
   ],
   "source": [
    "key = '0x6cf6296a'\n",
    "d = dialogue_data[key]\n",
    "pprint(d['quality'])\n",
    "pprint(list((idx, u) for idx,u in enumerate(d['utterances'])))\n",
    "pprint({k:[(idx,'__'.join(u)) for idx,u in enumerate(v)] for k,v in d['predictions'].items() if 'best_words' in k})\n",
    "pprint(dialogue_scores[dialogue_scores.index == key].to_dict('list'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
