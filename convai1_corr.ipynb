{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosko/miniconda3/envs/temp_lm_eval/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import qgrid\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, wasserstein_distance\n",
    "import json, bz2, pickle\n",
    "from pprint import pprint\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2154"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with bz2.open('./convai1_results.pickle.bz2') as fin:\n",
    "    convai1_data = pickle.load(fin)\n",
    "len(convai1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d21b63c79b841f599f48bb40834dfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2154.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dialogue_scores = list()\n",
    "indices = list()\n",
    "dialogue_data = dict()\n",
    "\n",
    "for d in tqdm(convai1_data):\n",
    "    d_item = dict()\n",
    "    dialogue_data[str(d['dialogId'])] = d\n",
    "    indices.append(str(d['dialogId']))\n",
    "    d_item['quality'] = d['quality']\n",
    "    \n",
    "    pred_keys = list(d['predictions'].keys())\n",
    "    bert_keys = list(filter(lambda x: 'bert' in x, pred_keys))\n",
    "    \n",
    "    for pred_key in bert_keys:\n",
    "        pred_sum = sum([np.log(x) for x in d['predictions'][pred_key] if x != 0])\n",
    "        pred_avg = pred_sum / len(d['predictions'][pred_key])\n",
    "        d_item['{}_log_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_log_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        pred_sum = sum(d['predictions'][pred_key])\n",
    "        pred_avg = pred_sum / len(d['predictions'][pred_key])\n",
    "        d_item['{}_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        d_item['{}_prd'.format(pred_key)] = np.prod(d['predictions'][pred_key])        \n",
    "        d_item['{}_prd_avg'.format(pred_key)] = d_item['{}_prd'.format(pred_key)] / len(d['predictions'][pred_key])\n",
    "        \n",
    "    prob_keys = list(filter(lambda x: 'prob' in x and x not in bert_keys, pred_keys))\n",
    "    \n",
    "    for pred_key in prob_keys: \n",
    "        s_sums = [sum(x) for x in d['predictions'][pred_key]]\n",
    "        s_sums_d_sum = sum([x for x in s_sums if x != 0])\n",
    "        s_sums_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_sums_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_sums_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_sums = [sum([np.log(x_1) for x_1 in x if x_1 != 0]) for x in d['predictions'][pred_key]]\n",
    "        s_sums_d_sum = sum([x for x in s_sums if x != 0])\n",
    "        s_sums_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_log_sums_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_log_sums_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_prd = [np.prod(x) for x in d['predictions'][pred_key]]\n",
    "        s_prd_d_sum = sum(s_sums)\n",
    "        s_prd_d_avg = s_sums_d_sum / len(s_sums)\n",
    "        \n",
    "        d_item['{}_s_prod_d_sum'.format(pred_key)] = pred_sum\n",
    "        d_item['{}_s_prod_d_avg'.format(pred_key)] = pred_avg\n",
    "        \n",
    "        s_avg = [float(sum([np.log(x_1) for x_1 in x if x_1 != 0]) / len(x)) for x in d['predictions'][pred_key] if len(x) > 0]        \n",
    "        s_avg_d_sum = sum(s_avg)\n",
    "        s_avg_d_avg = s_avg_d_sum / len(s_avg)        \n",
    "        \n",
    "        d_item['{}_s_log_avg_d_sum'.format(pred_key)] = s_avg_d_sum\n",
    "        d_item['{}_s_log_avg_d_avg'.format(pred_key)] = s_avg_d_avg\n",
    "        \n",
    "        s_avg = [float(sum(x) / len(x)) for x in d['predictions'][pred_key] if len(x) > 0]        \n",
    "        s_avg_d_sum = sum(s_avg)\n",
    "        s_avg_d_avg = s_avg_d_sum / len(s_avg)        \n",
    "        \n",
    "        d_item['{}_s_avg_d_sum'.format(pred_key)] = s_avg_d_sum\n",
    "        d_item['{}_s_avg_d_avg'.format(pred_key)] = s_avg_d_avg\n",
    "        \n",
    "    dialogue_scores.append(d_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-749262821</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.000357</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>3.999643</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>9.996427e-01</td>\n",
       "      <td>2.499107e-01</td>\n",
       "      <td>-40.210713</td>\n",
       "      <td>-10.052678</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-6.364283</td>\n",
       "      <td>-1.591071</td>\n",
       "      <td>0.978610</td>\n",
       "      <td>0.244652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-155769874</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.073824</td>\n",
       "      <td>-0.012304</td>\n",
       "      <td>5.928790</td>\n",
       "      <td>0.988132</td>\n",
       "      <td>9.288350e-01</td>\n",
       "      <td>1.548058e-01</td>\n",
       "      <td>-51.520240</td>\n",
       "      <td>-8.586707</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030035</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.030035</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>0.030035</td>\n",
       "      <td>0.005006</td>\n",
       "      <td>-8.110848</td>\n",
       "      <td>-1.351808</td>\n",
       "      <td>2.073650</td>\n",
       "      <td>0.345608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327080259</th>\n",
       "      <td>1.5</td>\n",
       "      <td>-15.297059</td>\n",
       "      <td>-1.019804</td>\n",
       "      <td>12.062486</td>\n",
       "      <td>0.804166</td>\n",
       "      <td>2.272855e-07</td>\n",
       "      <td>1.515237e-08</td>\n",
       "      <td>-115.073049</td>\n",
       "      <td>-7.671537</td>\n",
       "      <td>2.937514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.512429</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>1.512429</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>1.512429</td>\n",
       "      <td>0.100829</td>\n",
       "      <td>-20.992727</td>\n",
       "      <td>-1.399515</td>\n",
       "      <td>4.736203</td>\n",
       "      <td>0.315747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1682987452</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-18.694987</td>\n",
       "      <td>-1.699544</td>\n",
       "      <td>8.993401</td>\n",
       "      <td>0.817582</td>\n",
       "      <td>7.600989e-09</td>\n",
       "      <td>6.909990e-10</td>\n",
       "      <td>-83.706999</td>\n",
       "      <td>-7.609727</td>\n",
       "      <td>2.006599</td>\n",
       "      <td>...</td>\n",
       "      <td>2.006208</td>\n",
       "      <td>0.182383</td>\n",
       "      <td>2.006208</td>\n",
       "      <td>0.182383</td>\n",
       "      <td>2.006208</td>\n",
       "      <td>0.182383</td>\n",
       "      <td>-13.923161</td>\n",
       "      <td>-1.265742</td>\n",
       "      <td>3.801247</td>\n",
       "      <td>0.345568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037906078</th>\n",
       "      <td>2.5</td>\n",
       "      <td>-0.002120</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>5.997881</td>\n",
       "      <td>0.999647</td>\n",
       "      <td>9.978820e-01</td>\n",
       "      <td>1.663137e-01</td>\n",
       "      <td>-51.210174</td>\n",
       "      <td>-8.535029</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>-9.343397</td>\n",
       "      <td>-1.557233</td>\n",
       "      <td>1.896593</td>\n",
       "      <td>0.316099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             quality  bert-base-uncased_nsp_0_log_sum  \\\n",
       "-749262821       1.5                        -0.000357   \n",
       "-155769874       0.5                        -0.073824   \n",
       "1327080259       1.5                       -15.297059   \n",
       "-1682987452      0.5                       -18.694987   \n",
       "2037906078       2.5                        -0.002120   \n",
       "\n",
       "             bert-base-uncased_nsp_0_log_avg  bert-base-uncased_nsp_0_sum  \\\n",
       "-749262821                         -0.000089                     3.999643   \n",
       "-155769874                         -0.012304                     5.928790   \n",
       "1327080259                         -1.019804                    12.062486   \n",
       "-1682987452                        -1.699544                     8.993401   \n",
       "2037906078                         -0.000353                     5.997881   \n",
       "\n",
       "             bert-base-uncased_nsp_0_avg  bert-base-uncased_nsp_0_prd  \\\n",
       "-749262821                      0.999911                 9.996427e-01   \n",
       "-155769874                      0.988132                 9.288350e-01   \n",
       "1327080259                      0.804166                 2.272855e-07   \n",
       "-1682987452                     0.817582                 7.600989e-09   \n",
       "2037906078                      0.999647                 9.978820e-01   \n",
       "\n",
       "             bert-base-uncased_nsp_0_prd_avg  bert-base-uncased_nsp_1_log_sum  \\\n",
       "-749262821                      2.499107e-01                       -40.210713   \n",
       "-155769874                      1.548058e-01                       -51.520240   \n",
       "1327080259                      1.515237e-08                      -115.073049   \n",
       "-1682987452                     6.909990e-10                       -83.706999   \n",
       "2037906078                      1.663137e-01                       -51.210174   \n",
       "\n",
       "             bert-base-uncased_nsp_1_log_avg  bert-base-uncased_nsp_1_sum  \\\n",
       "-749262821                        -10.052678                     0.000357   \n",
       "-155769874                         -8.586707                     0.071210   \n",
       "1327080259                         -7.671537                     2.937514   \n",
       "-1682987452                        -7.609727                     2.006599   \n",
       "2037906078                         -8.535029                     0.002119   \n",
       "\n",
       "             ...  gpt2-large_sentences_best_word_probs_s_sums_d_sum  \\\n",
       "-749262821   ...                                           0.000054   \n",
       "-155769874   ...                                           0.030035   \n",
       "1327080259   ...                                           1.512429   \n",
       "-1682987452  ...                                           2.006208   \n",
       "2037906078   ...                                           0.003460   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_sums_d_avg  \\\n",
       "-749262821                                            0.000014   \n",
       "-155769874                                            0.005006   \n",
       "1327080259                                            0.100829   \n",
       "-1682987452                                           0.182383   \n",
       "2037906078                                            0.000577   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_sums_d_sum  \\\n",
       "-749262821                                            0.000054       \n",
       "-155769874                                            0.030035       \n",
       "1327080259                                            1.512429       \n",
       "-1682987452                                           2.006208       \n",
       "2037906078                                            0.003460       \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_sums_d_avg  \\\n",
       "-749262821                                            0.000014       \n",
       "-155769874                                            0.005006       \n",
       "1327080259                                            0.100829       \n",
       "-1682987452                                           0.182383       \n",
       "2037906078                                            0.000577       \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_prod_d_sum  \\\n",
       "-749262821                                            0.000054   \n",
       "-155769874                                            0.030035   \n",
       "1327080259                                            1.512429   \n",
       "-1682987452                                           2.006208   \n",
       "2037906078                                            0.003460   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_prod_d_avg  \\\n",
       "-749262821                                            0.000014   \n",
       "-155769874                                            0.005006   \n",
       "1327080259                                            0.100829   \n",
       "-1682987452                                           0.182383   \n",
       "2037906078                                            0.000577   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_avg_d_sum  \\\n",
       "-749262821                                           -6.364283      \n",
       "-155769874                                           -8.110848      \n",
       "1327080259                                          -20.992727      \n",
       "-1682987452                                         -13.923161      \n",
       "2037906078                                           -9.343397      \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_avg_d_avg  \\\n",
       "-749262821                                           -1.591071      \n",
       "-155769874                                           -1.351808      \n",
       "1327080259                                           -1.399515      \n",
       "-1682987452                                          -1.265742      \n",
       "2037906078                                           -1.557233      \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_avg_d_sum  \\\n",
       "-749262821                                           0.978610   \n",
       "-155769874                                           2.073650   \n",
       "1327080259                                           4.736203   \n",
       "-1682987452                                          3.801247   \n",
       "2037906078                                           1.896593   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_avg_d_avg  \n",
       "-749262821                                           0.244652  \n",
       "-155769874                                           0.345608  \n",
       "1327080259                                           0.315747  \n",
       "-1682987452                                          0.345568  \n",
       "2037906078                                           0.316099  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_scores = pd.DataFrame(dialogue_scores)\n",
    "dialogue_scores.index = indices\n",
    "dialogue_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_sum</th>\n",
       "      <th>bert-base-uncased_nsp_0_avg</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd</th>\n",
       "      <th>bert-base-uncased_nsp_0_prd_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_sum</th>\n",
       "      <th>bert-base-uncased_nsp_1_log_avg</th>\n",
       "      <th>bert-base-uncased_nsp_1_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_sums_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_prod_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_log_avg_d_avg</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_sum</th>\n",
       "      <th>gpt2-large_sentences_best_word_probs_s_avg_d_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-749262821</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.044356</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>9.996593e-01</td>\n",
       "      <td>4.998296e-01</td>\n",
       "      <td>0.946148</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.976474</td>\n",
       "      <td>0.394803</td>\n",
       "      <td>0.029466</td>\n",
       "      <td>0.185227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-155769874</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.998540</td>\n",
       "      <td>0.065752</td>\n",
       "      <td>0.988135</td>\n",
       "      <td>9.288504e-01</td>\n",
       "      <td>3.096168e-01</td>\n",
       "      <td>0.931001</td>\n",
       "      <td>0.306823</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.968154</td>\n",
       "      <td>0.505829</td>\n",
       "      <td>0.069868</td>\n",
       "      <td>0.317857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327080259</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.911509</td>\n",
       "      <td>0.878952</td>\n",
       "      <td>0.133782</td>\n",
       "      <td>0.804123</td>\n",
       "      <td>2.272893e-07</td>\n",
       "      <td>3.030523e-08</td>\n",
       "      <td>0.845887</td>\n",
       "      <td>0.380704</td>\n",
       "      <td>0.101861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.100903</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.100903</td>\n",
       "      <td>0.062299</td>\n",
       "      <td>0.100903</td>\n",
       "      <td>0.906785</td>\n",
       "      <td>0.483691</td>\n",
       "      <td>0.168104</td>\n",
       "      <td>0.278627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1682987452</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891852</td>\n",
       "      <td>0.798268</td>\n",
       "      <td>0.099743</td>\n",
       "      <td>0.817543</td>\n",
       "      <td>7.601115e-09</td>\n",
       "      <td>1.382021e-09</td>\n",
       "      <td>0.887895</td>\n",
       "      <td>0.385694</td>\n",
       "      <td>0.069581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082638</td>\n",
       "      <td>0.182519</td>\n",
       "      <td>0.082638</td>\n",
       "      <td>0.182519</td>\n",
       "      <td>0.082638</td>\n",
       "      <td>0.182519</td>\n",
       "      <td>0.940464</td>\n",
       "      <td>0.545767</td>\n",
       "      <td>0.133608</td>\n",
       "      <td>0.317804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037906078</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999959</td>\n",
       "      <td>0.066519</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>9.978985e-01</td>\n",
       "      <td>3.326328e-01</td>\n",
       "      <td>0.931416</td>\n",
       "      <td>0.310995</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.962282</td>\n",
       "      <td>0.410505</td>\n",
       "      <td>0.063335</td>\n",
       "      <td>0.279089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              quality  bert-base-uncased_nsp_0_log_sum  \\\n",
       "-749262821   0.222222                         0.999998   \n",
       "-155769874   0.000000                         0.999573   \n",
       "1327080259   0.222222                         0.911509   \n",
       "-1682987452  0.000000                         0.891852   \n",
       "2037906078   0.444444                         0.999988   \n",
       "\n",
       "             bert-base-uncased_nsp_0_log_avg  bert-base-uncased_nsp_0_sum  \\\n",
       "-749262821                          0.999990                     0.044356   \n",
       "-155769874                          0.998540                     0.065752   \n",
       "1327080259                          0.878952                     0.133782   \n",
       "-1682987452                         0.798268                     0.099743   \n",
       "2037906078                          0.999959                     0.066519   \n",
       "\n",
       "             bert-base-uncased_nsp_0_avg  bert-base-uncased_nsp_0_prd  \\\n",
       "-749262821                      0.999917                 9.996593e-01   \n",
       "-155769874                      0.988135                 9.288504e-01   \n",
       "1327080259                      0.804123                 2.272893e-07   \n",
       "-1682987452                     0.817543                 7.601115e-09   \n",
       "2037906078                      0.999653                 9.978985e-01   \n",
       "\n",
       "             bert-base-uncased_nsp_0_prd_avg  bert-base-uncased_nsp_1_log_sum  \\\n",
       "-749262821                      4.998296e-01                         0.946148   \n",
       "-155769874                      3.096168e-01                         0.931001   \n",
       "1327080259                      3.030523e-08                         0.845887   \n",
       "-1682987452                     1.382021e-09                         0.887895   \n",
       "2037906078                      3.326328e-01                         0.931416   \n",
       "\n",
       "             bert-base-uncased_nsp_1_log_avg  bert-base-uncased_nsp_1_sum  \\\n",
       "-749262821                          0.188477                     0.000012   \n",
       "-155769874                          0.306823                     0.002469   \n",
       "1327080259                          0.380704                     0.101861   \n",
       "-1682987452                         0.385694                     0.069581   \n",
       "2037906078                          0.310995                     0.000073   \n",
       "\n",
       "             ...  gpt2-large_sentences_best_word_probs_s_sums_d_sum  \\\n",
       "-749262821   ...                                           0.000002   \n",
       "-155769874   ...                                           0.001237   \n",
       "1327080259   ...                                           0.062299   \n",
       "-1682987452  ...                                           0.082638   \n",
       "2037906078   ...                                           0.000142   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_sums_d_avg  \\\n",
       "-749262821                                            0.000011   \n",
       "-155769874                                            0.005007   \n",
       "1327080259                                            0.100903   \n",
       "-1682987452                                           0.182519   \n",
       "2037906078                                            0.000575   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_sums_d_sum  \\\n",
       "-749262821                                            0.000002       \n",
       "-155769874                                            0.001237       \n",
       "1327080259                                            0.062299       \n",
       "-1682987452                                           0.082638       \n",
       "2037906078                                            0.000142       \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_sums_d_avg  \\\n",
       "-749262821                                            0.000011       \n",
       "-155769874                                            0.005007       \n",
       "1327080259                                            0.100903       \n",
       "-1682987452                                           0.182519       \n",
       "2037906078                                            0.000575       \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_prod_d_sum  \\\n",
       "-749262821                                            0.000002   \n",
       "-155769874                                            0.001237   \n",
       "1327080259                                            0.062299   \n",
       "-1682987452                                           0.082638   \n",
       "2037906078                                            0.000142   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_prod_d_avg  \\\n",
       "-749262821                                            0.000011   \n",
       "-155769874                                            0.005007   \n",
       "1327080259                                            0.100903   \n",
       "-1682987452                                           0.182519   \n",
       "2037906078                                            0.000575   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_avg_d_sum  \\\n",
       "-749262821                                            0.976474      \n",
       "-155769874                                            0.968154      \n",
       "1327080259                                            0.906785      \n",
       "-1682987452                                           0.940464      \n",
       "2037906078                                            0.962282      \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_log_avg_d_avg  \\\n",
       "-749262821                                            0.394803      \n",
       "-155769874                                            0.505829      \n",
       "1327080259                                            0.483691      \n",
       "-1682987452                                           0.545767      \n",
       "2037906078                                            0.410505      \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_avg_d_sum  \\\n",
       "-749262821                                           0.029466   \n",
       "-155769874                                           0.069868   \n",
       "1327080259                                           0.168104   \n",
       "-1682987452                                          0.133608   \n",
       "2037906078                                           0.063335   \n",
       "\n",
       "             gpt2-large_sentences_best_word_probs_s_avg_d_avg  \n",
       "-749262821                                           0.185227  \n",
       "-155769874                                           0.317857  \n",
       "1327080259                                           0.278627  \n",
       "-1682987452                                          0.317804  \n",
       "2037906078                                           0.279089  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in dialogue_scores.columns:\n",
    "    dialogue_scores[col] = minmax_scale(dialogue_scores[col])\n",
    "    \n",
    "dialogue_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72acd6937cc498a95440c9e3ff25ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': False, 'defa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((predictions-targets)**2))\n",
    "\n",
    "all_scores = {col:dict() for col in dialogue_scores.columns[1:]}\n",
    "\n",
    "for col in dialogue_scores.columns[1:]:\n",
    "    for f in (pearsonr, spearmanr, wasserstein_distance, rmse):\n",
    "        scores = f(dialogue_scores.quality, dialogue_scores[col])\n",
    "        if np.isscalar(scores):\n",
    "            scores = [scores]\n",
    "        \n",
    "        for score, name in zip(scores, [f.__name__, f.__name__+'_p']):\n",
    "            all_scores[col][name] = round(score, 3)\n",
    "\n",
    "all_scores = pd.DataFrame.from_dict(all_scores, orient='index')\n",
    "qgrid.show_grid(all_scores,\n",
    "               grid_options={\n",
    "    # SlickGrid options\n",
    "    'fullWidthRows': True,\n",
    "    'syncColumnCellResize': True,\n",
    "    'forceFitColumns': False,\n",
    "    'defaultColumnWidth': 80,\n",
    "    'rowHeight': 28,\n",
    "    'enableColumnReorder': False,\n",
    "    'enableTextSelectionOnCells': True,\n",
    "    'editable': True,\n",
    "    'autoEdit': False,\n",
    "    'explicitInitialization': True,\n",
    "\n",
    "    # Qgrid options\n",
    "    'maxVisibleRows': 15,\n",
    "    'minVisibleRows': 8,\n",
    "    'sortable': True,\n",
    "    'filterable': True,\n",
    "    'highlightSelectedCell': False,\n",
    "    'highlightSelectedRow': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[(0, 'Do you know Utrecht?'),\n",
      " (1, 'granted the right to accept only one religion'),\n",
      " (2, 'What do you mean?'),\n",
      " (3, 'granted the right to accept only one religion'),\n",
      " (4, 'Oh no bring me more pastes'),\n",
      " (5, 'Calvinism seems like a nice place!\\n'),\n",
      " (6, 'I dont think so')]\n",
      "{'gpt2-large_sentences_best_words': [(0,\n",
      "                                      'ind__,__Ġtitle__Ġto__Ġuse__Ġdonations__ĠDutch__Ġapplication__,'),\n",
      "                                     (1, 'Ġis__Ġyou__Ġthink__Ġby__ĠI'),\n",
      "                                     (2,\n",
      "                                      'anted__,__Ġfact__Ġto__Ġbe__Ġor__Ġthe__Ġof__,'),\n",
      "                                     (3, ',__,__Ġback__Ġthe__Ġof__ries__!'),\n",
      "                                     (4,\n",
      "                                      'm__Ġand__Ġis__Ġto__Ġa__Ġgood__Ġidea__Ġto__ĠI'),\n",
      "                                     (5, \"'m__Ġknow__Ġit__.\")],\n",
      " 'gpt2-medium_sentences_best_words': [(0,\n",
      "                                       '?__.__Ġright__Ġto__Ġvote__Ġor__ĠDutch__Ġapplication__?'),\n",
      "                                      (1, 'Ġis__Ġyou__Ġmean__Ġby__Ċ'),\n",
      "                                      (2,\n",
      "                                       'ace__,__Ġsame__Ġto__Ġbe__Ġor__Ġthose__Ġperson__?'),\n",
      "                                      (3, ',__,__Ġme__Ġback__Ġof__ries__.'),\n",
      "                                      (4,\n",
      "                                       'm__Ġand__Ġis__Ġto__Ġa__Ġgood__Ġidea__Ġto__ĠI'),\n",
      "                                      (5, \"'m__Ġknow__Ġit__.\")],\n",
      " 'gpt2_sentences_best_words': [(0,\n",
      "                                'ace__,__Ġcity__Ġto__Ġvote__Ġthe__Ġthose__Ġof__,'),\n",
      "                               (1, 'Ġis__Ġyou__Ġthink__Ġby__ĠI'),\n",
      "                               (2,\n",
      "                                'ace__,__Ġgame__Ġto__Ġvote__Ġthe__Ġthose__Ġof__,'),\n",
      "                               (3, ',__,__Ġit__Ġback__Ġthan__ries__,'),\n",
      "                               (4,\n",
      "                                \"ories__'s__,__Ġto__Ġa__Ġgood__Ġidea__Ġto__Ċ\"),\n",
      "                               (5, \"'m__Ġknow__Ġit__.\")],\n",
      " 'xlnet-base-cased_sentences_best_words': [(0,\n",
      "                                            '.__▁granted__▁the__▁to__▁to__▁accept__▁in__▁of'),\n",
      "                                           (1,\n",
      "                                            '▁If__▁What__▁what__▁think__▁by'),\n",
      "                                           (2,\n",
      "                                            '<eop>__▁granted__▁one__▁to__▁to__▁a__▁in__▁'),\n",
      "                                           (3,\n",
      "                                            '▁If__▁Oh__▁no__▁no__▁me__▁in__▁the'),\n",
      "                                           (4,\n",
      "                                            '<eop>__▁no__▁bring__▁to__▁a__▁__,__▁to'),\n",
      "                                           (5, '<eop>__▁I__▁not__t__▁what')],\n",
      " 'xlnet-large-cased_sentences_best_words': [(0,\n",
      "                                             '▁piece__▁of__▁to__▁and__▁to__▁for__▁one__▁of'),\n",
      "                                            (1, '▁all__▁What__▁you__▁do__?'),\n",
      "                                            (2,\n",
      "                                             '▁on__<eop>__▁same__▁right__▁to__▁office__▁by__▁and'),\n",
      "                                            (3,\n",
      "                                             '▁all__▁Oh__,__▁bring__▁me__.__.'),\n",
      "                                            (4,\n",
      "                                             '▁__▁No__ism__▁it__▁an__▁__▁__▁to'),\n",
      "                                            (5, '▁__▁I__,__t__▁think')]}\n",
      "{'bert-base-uncased_nsp_0_avg': [0.17543521891382868],\n",
      " 'bert-base-uncased_nsp_0_log_avg': [0.3692100178823261],\n",
      " 'bert-base-uncased_nsp_0_log_sum': [0.815547237651195],\n",
      " 'bert-base-uncased_nsp_0_prd': [1.4201656412256871e-14],\n",
      " 'bert-base-uncased_nsp_0_prd_avg': [4.73388547075229e-15],\n",
      " 'bert-base-uncased_nsp_0_sum': [0.011682545118655756],\n",
      " 'bert-base-uncased_nsp_1_avg': [0.824564763649976],\n",
      " 'bert-base-uncased_nsp_1_log_avg': [0.8938508674405777],\n",
      " 'bert-base-uncased_nsp_1_log_sum': [0.9894328305026677],\n",
      " 'bert-base-uncased_nsp_1_prd': [0.00037437321984713453],\n",
      " 'bert-base-uncased_nsp_1_prd_avg': [0.00012479107328237818],\n",
      " 'bert-base-uncased_nsp_1_sum': [0.17151540166700932],\n",
      " 'bert-large-uncased_nsp_0_avg': [0.2029225052981121],\n",
      " 'bert-large-uncased_nsp_0_log_avg': [0.31833790592676425],\n",
      " 'bert-large-uncased_nsp_0_log_sum': [0.7775109780608735],\n",
      " 'bert-large-uncased_nsp_0_prd': [1.3789676877983687e-13],\n",
      " 'bert-large-uncased_nsp_0_prd_avg': [4.5965589593278953e-14],\n",
      " 'bert-large-uncased_nsp_0_sum': [0.012876084030345935],\n",
      " 'bert-large-uncased_nsp_1_avg': [0.7970774905478474],\n",
      " 'bert-large-uncased_nsp_1_log_avg': [0.9053442020256735],\n",
      " 'bert-large-uncased_nsp_1_log_sum': [0.991465543981414],\n",
      " 'bert-large-uncased_nsp_1_prd': [0.0006319237215271942],\n",
      " 'bert-large-uncased_nsp_1_prd_avg': [0.00021064124050906472],\n",
      " 'bert-large-uncased_nsp_1_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_avg': [0.20519728188287376],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_sum': [0.05088404311950158],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_avg': [0.36757258552126115],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_sum': [0.9596372345869455],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_avg': [0.0005362729180092752],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_sum': [0.00020947462384865502],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_avg': [0.329334146136955],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_sum': [0.9490001707882261],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_avg': [0.1991193483385375],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_sum': [0.05244227897989144],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_avg': [0.42769763097402236],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_sum': [0.9571537079187575],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_avg': [0.001716774534028455],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_sum': [0.0006592692682139806],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_avg': [0.3533435737276107],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_sum': [0.9497466245624788],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_avg': [0.19483245184541653],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_sum': [0.050521163121966875],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_avg': [0.4062698947834501],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_sum': [0.9590015702587084],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_avg': [0.0009982750846559395],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_sum': [0.0003864960454546188],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_avg': [0.3267427792705172],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_sum': [0.9510727991670779],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'quality': [0.0],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_avg': [0.3794879085203452],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_sum': [0.043868366469977625],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.5547680291192406],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9582626293110036],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_avg': [0.06574490336726138],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_sum': [0.02354524486264329],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_avg': [0.4779630254673569],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_sum': [0.9630743978980301],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_avg': [0.4244893478177689],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_sum': [0.04101338975592547],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.5726723879618646],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9483595529071079],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_avg': [0.1381258853519155],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_sum': [0.03123213303965593],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_avg': [0.6322358653238311],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_sum': [0.9660941569266963],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_sum': [0.1968462918389412],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_avg': [0.7970774905478474],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_sum': [0.1968462918389412]}\n"
     ]
    }
   ],
   "source": [
    "key = '-1286395059'\n",
    "d = dialogue_data[key]\n",
    "pprint(d['quality'])\n",
    "pprint(list((idx, u) for idx,u in enumerate(d['utterances'])))\n",
    "pprint({k:[(idx,'__'.join(u)) for idx,u in enumerate(v)] for k,v in d['predictions'].items() if 'best_words' in k})\n",
    "pprint(dialogue_scores[dialogue_scores.index == key].to_dict('list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "[(0, 'fascinating :D'),\n",
      " (1, 'What do you find interesting about this region?'),\n",
      " (2, 'I hadn\\'t heard of the concept of a \"fall line\" before. Had you?'),\n",
      " (3, 'Neither had I.'),\n",
      " (4, \"Other than that, i don't find the snippet very interesting.\"),\n",
      " (5, 'have you been to raleigh?'),\n",
      " (6, 'No, I have not. And you?'),\n",
      " (7, 'No.'),\n",
      " (8, 'I think it would be great to visit this place for leasure.'),\n",
      " (9, 'Yea sounds like there might be some good hiking around there.')]\n",
      "{'gpt2-large_sentences_best_words': [(0,\n",
      "                                      \"'s__Ġyou__Ġthink__Ġmost__Ġabout__Ġthe__Ġgame__?__Ċ\"),\n",
      "                                     (1,\n",
      "                                      '\\'m__\\'t__Ġheard__Ġof__Ġthis__Ġregion__Ġof__Ġthe__Ġ\"__C__en__\"__Ġbefore__,__ĠI__Ġyou__?__ĠI'),\n",
      "                                     (2, 'Ġhad__ĠI__.__ĠBut'),\n",
      "                                     (3,\n",
      "                                      \"Ġthan__Ġthe__,__ĠI__Ġwas__'t__Ġthink__Ġit__Ġgame__Ġto__Ġinteresting__.__ĠI\"),\n",
      "                                     (4, 'Ġyou__Ġtried__Ġto__Ġthe__/__?__Ġi'),\n",
      "                                     (5,\n",
      "                                      ',__ĠI__Ġhaven__Ġnot__.__ĠI__ĠI__Ġknow__No'),\n",
      "                                     (6, ',__ĠI'),\n",
      "                                     (7,\n",
      "                                      \"'m__Ġit__'s__Ġbe__Ġa__Ġif__Ġhave__Ġthe__Ġplace__.__Ġa__m__.__ĠI\"),\n",
      "                                     (8,\n",
      "                                      'ours__,__Ġlike__Ġa__Ġis__Ġbe__Ġa__Ġgood__Ġthings__Ġin__Ġhere__.__Ċ')],\n",
      " 'gpt2-medium_sentences_best_words': [(0,\n",
      "                                       'Ġa__Ġyou__Ġthink__Ġinteresting__Ġabout__Ġthe__?__?__ĠI'),\n",
      "                                      (1,\n",
      "                                       '\\'m__\\'t__Ġthought__Ġof__Ġit__Ġregion__Ġof__Ġthe__Ġ\"__C__en__\"__Ġbefore__,__ĠI__Ġyou__Ġseen__I'),\n",
      "                                      (2, 'Ġhad__ĠI__.__ĠBut'),\n",
      "                                      (3,\n",
      "                                       \"Ġthan__Ġthe__,__ĠI__'m__'t__Ġknow__Ġit__Ġwhole__Ġof__Ġinteresting__.__ĠI\"),\n",
      "                                      (4,\n",
      "                                       'Ġyou__Ġtried__Ġable__Ġthe__/__?__Ġi'),\n",
      "                                      (5,\n",
      "                                       ',__ĠI__Ġhaven__Ġnot__.__ĠI__ĠI__Ġknow__I'),\n",
      "                                      (6, ',__ĠI'),\n",
      "                                      (7,\n",
      "                                       \"'m__Ġit__'s__Ġbe__Ġa__Ġif__Ġhave__Ġthe__Ġplace__.__Ġa__mons__.__ĠI\"),\n",
      "                                      (8,\n",
      "                                       'a__,__Ġlike__Ġa__Ġis__Ġbe__Ġa__Ġgood__Ġfood__Ġin__Ġthe__.__Ċ')],\n",
      " 'gpt2_sentences_best_words': [(0,\n",
      "                                'Ġis__Ġyou__Ġthink__Ġmost__Ġabout__Ġthe__Ġbook__?__Ċ'),\n",
      "                               (1,\n",
      "                                'Ġthink__\\'t__Ġheard__Ġof__Ġit__Ġregion__Ġof__Ġa__Ġ\"__region__en__\"__Ġbefore__,__ĠI__ĠI__Ġheard__ĠI'),\n",
      "                               (2, 'Ġdid__ĠI__.__ĠI'),\n",
      "                               (3,\n",
      "                                \".__Ġthat__,__ĠI__Ġwas__'t__Ġthink__Ġit__Ġstory__Ġof__Ġinteresting__.__ĠI\"),\n",
      "                               (4, 'Ġa__Ġtried__Ġplaying__Ġthe__/__?__Ċ'),\n",
      "                               (5, ',__ĠI__Ġhaven__Ġnot__.__ĠI__ĠI__Ġknow__I'),\n",
      "                               (6, ',__ĠI'),\n",
      "                               (7,\n",
      "                                \".__Ġit__'s__Ġbe__Ġa__Ġif__Ġhave__Ġthe__Ġplace__.__Ġa__mons__.__ĠI\"),\n",
      "                               (8,\n",
      "                                \"ours__,__Ġlike__Ġa__'s__Ġbe__Ġa__Ġkind__Ġthings__Ġin__Ġhere__.__ĠI\")],\n",
      " 'xlnet-base-cased_sentences_best_words': [(0,\n",
      "                                            '▁3__▁What__▁what__▁do__▁fascinating__▁in__▁the__▁__▁of'),\n",
      "                                           (1,\n",
      "                                            '▁__▁do__▁you__t__▁been__▁that__▁this__▁__▁of__▁the__▁__\"__\"__-__\"__.__.__▁What__▁I__▁heard'),\n",
      "                                           (2, '▁__▁I__▁heard__▁heard'),\n",
      "                                           (3,\n",
      "                                            '<eop>__▁Other__▁other__▁that__,__\"__.__\\'__t__▁know__▁that__▁time__.__▁interesting__.'),\n",
      "                                           (4,\n",
      "                                            '▁Other__▁not__▁had__▁__▁the__i__.__?'),\n",
      "                                           (5,\n",
      "                                            '▁__▁No__▁no__▁have__▁you__▁been__.__▁you__▁have'),\n",
      "                                           (6, '▁The__,'),\n",
      "                                           (7,\n",
      "                                            '<eop>__▁I__▁I__▁was__▁take__▁a__.__▁have__▁to__▁one__,__▁the__\"__.__.'),\n",
      "                                           (8,\n",
      "                                            '<eod>__▁Ye__a__,__▁a__▁is__▁be__▁a__▁of__▁things__▁in__▁the__.')],\n",
      " 'xlnet-large-cased_sentences_best_words': [(0,\n",
      "                                             '▁R__▁So__▁do__▁do__▁__▁and__▁about__▁year__?'),\n",
      "                                            (1,\n",
      "                                             '▁__▁2__▁___t__▁land__▁it__▁a__▁United__▁of__▁the__▁__\"__\"__\"__\"__▁until__▁I__▁I__▁you__▁heard'),\n",
      "                                            (2, '▁I__▁Nor__▁heard__▁heard'),\n",
      "                                            (3,\n",
      "                                             '<eod>__▁Other__▁than__▁That__,__(__▁__t__t__▁know__▁it__▁__▁of__▁interesting__.'),\n",
      "                                            (4,\n",
      "                                             '▁__▁read__▁heard__▁doing__?__.__▁__?'),\n",
      "                                            (5,\n",
      "                                             '▁__▁No__,__,__▁been__▁been__▁No__▁no__▁have'),\n",
      "                                            (6, '<eod>__▁No'),\n",
      "                                            (7,\n",
      "                                             '<eop>__▁I__▁I__▁is__▁be__▁a__▁if__▁have__.__▁place__.__▁a__(__▁__.'),\n",
      "                                            (8,\n",
      "                                             '<eod>__▁Ye__a__▁like__▁a__▁is__▁be__▁some__▁good__▁stuff__▁around__▁here__.')]}\n",
      "{'bert-base-uncased_nsp_0_avg': [0.7774611149487617],\n",
      " 'bert-base-uncased_nsp_0_log_avg': [0.7960622496664601],\n",
      " 'bert-base-uncased_nsp_0_log_sum': [0.9105480032737803],\n",
      " 'bert-base-uncased_nsp_0_prd': [1.925105149606129e-07],\n",
      " 'bert-base-uncased_nsp_0_prd_avg': [4.278011443569176e-08],\n",
      " 'bert-base-uncased_nsp_0_sum': [0.07760652975441062],\n",
      " 'bert-base-uncased_nsp_1_avg': [0.22253888829936844],\n",
      " 'bert-base-uncased_nsp_1_log_avg': [0.4144517961281473],\n",
      " 'bert-base-uncased_nsp_1_log_sum': [0.9125712284902086],\n",
      " 'bert-base-uncased_nsp_1_prd': [4.454687256897248e-29],\n",
      " 'bert-base-uncased_nsp_1_prd_avg': [9.899305015327216e-30],\n",
      " 'bert-base-uncased_nsp_1_sum': [0.06943554868820763],\n",
      " 'bert-large-uncased_nsp_0_avg': [0.8881407736177536],\n",
      " 'bert-large-uncased_nsp_0_log_avg': [0.9058538001194425],\n",
      " 'bert-large-uncased_nsp_0_log_sum': [0.953907049192625],\n",
      " 'bert-large-uncased_nsp_0_prd': [0.002166243105061721],\n",
      " 'bert-large-uncased_nsp_0_prd_avg': [0.0004813873566803825],\n",
      " 'bert-large-uncased_nsp_0_sum': [0.08437891115568291],\n",
      " 'bert-large-uncased_nsp_1_avg': [0.11185919438645127],\n",
      " 'bert-large-uncased_nsp_1_log_avg': [0.40305693553559363],\n",
      " 'bert-large-uncased_nsp_1_log_sum': [0.9192937810447215],\n",
      " 'bert-large-uncased_nsp_1_prd': [5.564607083969861e-31],\n",
      " 'bert-large-uncased_nsp_1_prd_avg': [1.2365793519933024e-31],\n",
      " 'bert-large-uncased_nsp_1_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_avg': [0.32793823470243133],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_sum': [0.11066971180626059],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_avg': [0.527222105956243],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_sum': [0.9508104899607375],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_avg': [0.00028700322787624335],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_sum': [0.00017684361767065533],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_avg': [0.3026434189975541],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_sum': [0.916087569730416],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_avg': [0.29307964796318825],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_sum': [0.10894085131332275],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_avg': [0.5828440106788944],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_sum': [0.9489367427637847],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_avg': [0.00022963604100931597],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_sum': [0.00015397829709047782],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_avg': [0.29713509457565],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_sum': [0.9151027890582132],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_avg': [0.2883649548457091],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_sum': [0.10392886548866939],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_avg': [0.5527888161699938],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_sum': [0.950016133391186],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_avg': [0.0002322145954148144],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_sum': [0.0001505903302381337],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_avg': [0.2620733450798093],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_sum': [0.914824095539891],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'quality': [1.0],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_avg': [0.43707460626945294],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_sum': [0.07603683807446678],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.5436687694918968],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9340248254508717],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_avg': [0.16410333631922788],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_sum': [0.08813137530570679],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_avg': [0.6530927829779828],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_sum': [0.9604477326261017],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_avg': [0.5534925037933643],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_sum': [0.07870152397825131],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.7242968797572396],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9432239085804776],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_avg': [0.2584905709339871],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_sum': [0.0876707415431942],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_avg': [0.7538066580145815],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_sum': [0.9601135108493792],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_sum': [0.041437723989253536],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_avg': [0.11185919438645127],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_sum': [0.041437723989253536]}\n"
     ]
    }
   ],
   "source": [
    "key = '1111790167'\n",
    "d = dialogue_data[key]\n",
    "pprint(d['quality'])\n",
    "pprint(list((idx, u) for idx,u in enumerate(d['utterances'])))\n",
    "pprint({k:[(idx,'__'.join(u)) for idx,u in enumerate(v)] for k,v in d['predictions'].items() if 'best_words' in k})\n",
    "pprint(dialogue_scores[dialogue_scores.index == key].to_dict('list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[(0, 'Hi'),\n",
      " (1, 'Who uses the four stages of civil society ?'),\n",
      " (2, 'Ehh its incorrect. Hint: first 3 answer letters is \"fer\" '),\n",
      " (3, 'What is your name?'),\n",
      " (4, 'What'),\n",
      " (5, 'Please, speak with me.'),\n",
      " (6, 'Please, speak with me. It gives me energy to live')]\n",
      "{'gpt2-large_sentences_best_words': [(0,\n",
      "                                      'a__Ġthe__Ġsite__-__Ġof__Ġgrief__Ġdisobedience__?__Ċ'),\n",
      "                                     (1,\n",
      "                                      '.__,__Ġa__Ġto__ĠThe__aha__:__Ġit__,__Ġstages__Ġare__Ġare__Ġthe__civil__\"__Ġand'),\n",
      "                                     (2, 'Ġis__Ġthe__Ġfavorite__?__Ċ'),\n",
      "                                     (3, 'Ġis'),\n",
      "                                     (4, 'Ċ__Ġwhat__Ġto__Ġme__.__Ċ'),\n",
      "                                     (5,\n",
      "                                      \",__Ġspeak__Ġwith__Ġme__.__Ċ__'s__Ġme__Ġa__.__Ġkeep__.\")],\n",
      " 'gpt2-medium_sentences_best_words': [(0,\n",
      "                                       'a__Ġthis__Ġapp__-__Ġof__Ġgrief__Ġwar__?__Ċ'),\n",
      "                                      (1,\n",
      "                                       'mp__,__Ġa__Ġto__ĠThe__mmm__:__Ġit__Ġstage__Ġstages__Ġis__Ġare__Ġthe__civil__ret__Ġand'),\n",
      "                                      (2, 'Ġis__Ġthe__Ġname__?__Ċ'),\n",
      "                                      (3, 'Ġis'),\n",
      "                                      (4, 'Do__Ġa__Ġup__Ġme__.__Ċ'),\n",
      "                                      (5,\n",
      "                                       \",__Ġspeak__Ġwith__Ġme__.__Please__'s__Ġme__Ġa__.__Ġdo__.\")],\n",
      " 'gpt2_sentences_best_words': [(0,\n",
      "                                ',__Ġthe__Ġsame__-__Ġof__Ġthe__Ġdisobedience__?__Ċ'),\n",
      "                               (1,\n",
      "                                '.__,__Ġa__Ġto__ĠThe__aha__:__Ġit__,__Ġstages__Ġis__Ġare__Ġthe__E__ry__Ġand'),\n",
      "                               (2, 'Ġis__Ġthe__Ġname__?__Ċ'),\n",
      "                               (3, 'Ġis'),\n",
      "                               (4, 'Say__ĠI__Ġup__Ġme__.__Ċ'),\n",
      "                               (5,\n",
      "                                \",__Ġspeak__Ġwith__Ġme__.__Ċ__'s__Ġme__Ġthe__.__Ġthink__.\")],\n",
      " 'xlnet-base-cased_sentences_best_words': [(0,\n",
      "                                            '▁I__▁Who__▁or__▁the__▁the__▁stages__▁of__▁or__?__▁'),\n",
      "                                           (1,\n",
      "                                            '▁__▁E__h__h__▁its__.__▁Who__t__:__▁__▁stage__▁stages__▁is__▁__▁incorrect__\"__\"__\"'),\n",
      "                                           (2, '<eop>__▁is__▁a__▁name__?'),\n",
      "                                           (3, '<eop>'),\n",
      "                                           (4, '▁What__▁Please__,__,__,__▁me'),\n",
      "                                           (5,\n",
      "                                            '<eop>__▁Please__▁Please__▁with__▁me__.__<eop>__▁is__▁life__▁a__▁and__▁speak')],\n",
      " 'xlnet-large-cased_sentences_best_words': [(0,\n",
      "                                             '▁I__▁I__▁he__▁the__▁four__▁of__▁of__▁of__▁step__,'),\n",
      "                                            (1,\n",
      "                                             '▁__▁E__h__-__▁who__,__.__k__,__:__,__▁stages__▁is__▁are__▁correct__\"__▁Who__\"'),\n",
      "                                            (2, '▁__▁is__▁a__▁favorite__?'),\n",
      "                                            (3, '?'),\n",
      "                                            (4,\n",
      "                                             '▁is__▁many__▁room__,__▁with__▁with'),\n",
      "                                            (5,\n",
      "                                             '<eod>__▁Please__,__,__▁me__.__<eop>__▁is__▁me__▁a__.__▁speak')]}\n",
      "{'bert-base-uncased_nsp_0_avg': [0.988134727830006],\n",
      " 'bert-base-uncased_nsp_0_log_avg': [0.9985402422795526],\n",
      " 'bert-base-uncased_nsp_0_log_sum': [0.9995730334589272],\n",
      " 'bert-base-uncased_nsp_0_prd': [0.9288504335849298],\n",
      " 'bert-base-uncased_nsp_0_prd_avg': [0.3096168111949766],\n",
      " 'bert-base-uncased_nsp_0_sum': [0.06575222644553208],\n",
      " 'bert-base-uncased_nsp_1_avg': [0.011865264554092408],\n",
      " 'bert-base-uncased_nsp_1_log_avg': [0.3068232977623865],\n",
      " 'bert-base-uncased_nsp_1_log_sum': [0.9310012329425751],\n",
      " 'bert-base-uncased_nsp_1_prd': [4.219420869400401e-23],\n",
      " 'bert-base-uncased_nsp_1_prd_avg': [1.406473623133467e-23],\n",
      " 'bert-base-uncased_nsp_1_sum': [0.0024687147050379264],\n",
      " 'bert-large-uncased_nsp_0_avg': [0.9949926403758936],\n",
      " 'bert-large-uncased_nsp_0_log_avg': [0.9993040759531073],\n",
      " 'bert-large-uncased_nsp_0_log_sum': [0.9997727862005829],\n",
      " 'bert-large-uncased_nsp_0_prd': [0.9702115705385842],\n",
      " 'bert-large-uncased_nsp_0_prd_avg': [0.32340385684619477],\n",
      " 'bert-large-uncased_nsp_0_sum': [0.06301050666091416],\n",
      " 'bert-large-uncased_nsp_1_avg': [0.005007333767808692],\n",
      " 'bert-large-uncased_nsp_1_log_avg': [0.4118300858800825],\n",
      " 'bert-large-uncased_nsp_1_log_sum': [0.9469871281028507],\n",
      " 'bert-large-uncased_nsp_1_prd': [1.3394200300013546e-20],\n",
      " 'bert-large-uncased_nsp_1_prd_avg': [4.464733433337849e-21],\n",
      " 'bert-large-uncased_nsp_1_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_avg': [0.317857486523047],\n",
      " 'gpt2-large_sentences_best_word_probs_s_avg_d_sum': [0.06986773215244826],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_avg': [0.5058290593823173],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_avg_d_sum': [0.9681535658880835],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_best_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_best_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_best_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_avg': [0.00025671364386853764],\n",
      " 'gpt2-large_sentences_word_probs_s_avg_d_sum': [0.00010464323131515074],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_avg': [0.27428385451795056],\n",
      " 'gpt2-large_sentences_word_probs_s_log_avg_d_sum': [0.9449078102468323],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-large_sentences_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_avg': [0.2842846464198785],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_avg_d_sum': [0.06887155193846216],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_avg': [0.5484432278145552],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_avg_d_sum': [0.9656063495476942],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_best_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_avg': [0.0004124646327156548],\n",
      " 'gpt2-medium_sentences_word_probs_s_avg_d_sum': [0.00017017581199591907],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_avg': [0.2797896410222802],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_avg_d_sum': [0.9443757307410873],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2-medium_sentences_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_avg': [0.2784520019849462],\n",
      " 'gpt2_sentences_best_word_probs_s_avg_d_sum': [0.06548243301492386],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_avg': [0.5108576098814689],\n",
      " 'gpt2_sentences_best_word_probs_s_log_avg_d_sum': [0.9659391286619645],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_best_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_best_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_best_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_avg': [0.0003520310193713384],\n",
      " 'gpt2_sentences_word_probs_s_avg_d_sum': [0.00014416378211542204],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_avg': [0.26741810937443633],\n",
      " 'gpt2_sentences_word_probs_s_log_avg_d_sum': [0.9467663580588543],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'gpt2_sentences_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'quality': [0.0],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_avg': [0.43298070656175947],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_avg_d_sum': [0.048484477744013765],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.5789119788136912],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9600647002234467],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_best_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_avg': [0.1528864338569163],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_avg_d_sum': [0.05473718495407207],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_avg': [0.5243691286075626],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_avg_d_sum': [0.9662992474097879],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-base-cased_sentences_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_avg': [0.46302709783452933],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_avg_d_sum': [0.044068845809796295],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_avg': [0.6507005546513295],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_avg_d_sum': [0.9559675093233533],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_best_word_probs_s_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_avg': [0.22311268677418877],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_avg_d_sum': [0.05044787658051827],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_avg': [0.6913179123127637],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_avg_d_sum': [0.9703087659420098],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_log_sums_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_prod_d_sum': [0.0012369965259775525],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_avg': [0.005007333767808692],\n",
      " 'xlnet-large-cased_sentences_word_probs_s_sums_d_sum': [0.0012369965259775525]}\n"
     ]
    }
   ],
   "source": [
    "key = '-155769874'\n",
    "d = dialogue_data[key]\n",
    "pprint(d['quality'])\n",
    "pprint(list((idx, u) for idx,u in enumerate(d['utterances'])))\n",
    "pprint({k:[(idx,'__'.join(u)) for idx,u in enumerate(v)] for k,v in d['predictions'].items() if 'best_words' in k})\n",
    "pprint(dialogue_scores[dialogue_scores.index == key].to_dict('list'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
